{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4f0370b-99de-4660-9227-41d567bc251b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The purpose of a variational autoencoder (VAE) is to learn a compressed, latent representation of input data while ensuring that the learned representation is smooth and continuous. Unlike a standard autoencoder which simply aims to reconstruct the input, a VAE adds a crucial probabilistic layer.  This allows it to not only reconstruct the input but also generate new data samples similar to the training data.\n",
      "\n",
      "More specifically, the VAE aims to achieve the following:\n",
      "\n",
      "1. **Dimensionality Reduction:**  It compresses high-dimensional input data (like images, audio, or text) into a lower-dimensional latent space. This latent space captures the essential features of the data.\n",
      "\n",
      "2. **Learning a Probability Distribution:** Instead of simply mapping input data to a point in the latent space, the VAE learns a probability distribution over the latent space. This means for each input, it learns not just a single point but a distribution of likely latent representations. This is crucial for generating new data.\n",
      "\n",
      "3. **Data Generation:**  By sampling from the learned probability distribution in the latent space and then decoding it back to the original data space, the VAE can generate new data samples that resemble the training data. This is a key difference between VAEs and standard autoencoders.\n",
      "\n",
      "4. **Smooth and Continuous Latent Space:** The probabilistic nature of the VAE encourages a smooth and continuous latent space.  This means that small changes in the latent space lead to small, predictable changes in the generated data. This property is desirable for many applications, including interpolation and manipulation of data.\n",
      "\n",
      "In essence, the VAE learns a generative model of the data, allowing for both encoding and decoding, and importantly, the generation of new, similar data.  It's this generative capability, coupled with the smooth latent space, that sets it apart from other dimensionality reduction techniques.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Create a ChatOpenAI model\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "\n",
    "# Define prompt templates (no need for separate Runnable chains)\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a scientist who invented the {model}.\"),\n",
    "        (\"human\", \"Tell me what is that {model}'s purpose.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the combined chain using LangChain Expression Language (LCEL)\n",
    "chain = prompt_template | model | StrOutputParser()\n",
    "# chain = prompt_template | model\n",
    "\n",
    "# Run the chain\n",
    "result = chain.invoke({\"model\": \"variational autoencoder\", \"model\": 'variational autoencoder'})\n",
    "\n",
    "# Output\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn_langchain",
   "language": "python",
   "name": "learn_langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
