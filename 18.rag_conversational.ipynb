{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c798d19-128b-4344-b2f4-e01a35050426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Define the persistent directory\n",
    "current_dir = os.path.dirname(os.path.abspath('Learn-LangChain'))\n",
    "persistent_directory = os.path.join(current_dir, \"db\", \"chroma_db_with_metadata\")\n",
    "\n",
    "# Define the embedding model\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Load the existing vector store with the embedding function\n",
    "db = Chroma(persist_directory=persistent_directory, embedding_function=embeddings)\n",
    "\n",
    "# Create a retriever for querying the vector store\n",
    "# `search_type` specifies the type of search (e.g., similarity)\n",
    "# `search_kwargs` contains additional arguments for the search (e.g., number of results to return)\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3},\n",
    ")\n",
    "\n",
    "# Create a model\n",
    "llm = ChatGoogleGenerativeAI(model='gemini-1.5-flash')\n",
    "\n",
    "# Contextualize question prompt\n",
    "# This system prompt helps the AI understand that it should reformulate the question\n",
    "# based on the chat history to make it a standalone question\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, just \"\n",
    "    \"reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "# Create a prompt template for contextualizing questions\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a history-aware retriever\n",
    "# This uses the LLM to help reformulate the question based on chat history\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "# Answer question prompt\n",
    "# This system prompt helps the AI understand that it should provide concise answers\n",
    "# based on the retrieved context and indicates what to do if the answer is unknown\n",
    "qa_system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. Use \"\n",
    "    \"the following pieces of retrieved context to answer the \"\n",
    "    \"question. If you don't know the answer, just say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the answer \"\n",
    "    \"concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "# Create a prompt template for answering questions\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a chain to combine documents for question answering\n",
    "# `create_stuff_documents_chain` feeds all retrieved context into the LLM\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "# Create a retrieval chain that combines the history-aware retriever and the question answering chain\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "\n",
    "# Function to simulate a continual chat\n",
    "def continual_chat():\n",
    "    print(\"Start chatting with the AI! Type 'exit' to end the conversation.\")\n",
    "    chat_history = []  # Collect chat history here (a sequence of messages)\n",
    "    while True:\n",
    "        query = input(\"You: \")\n",
    "        if query.lower() == \"exit\":\n",
    "            break\n",
    "        # Process the user's query through the retrieval chain\n",
    "        result = rag_chain.invoke({\"input\": query, \"chat_history\": chat_history})\n",
    "        # Display the AI's response\n",
    "        print(f\"AI: {result['answer']}\")\n",
    "        # Update the chat history\n",
    "        chat_history.append(HumanMessage(content=query))\n",
    "        chat_history.append(AIMessage(content=result[\"answer\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "273a17f5-2987-4493-a665-b5b6912519b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBranch(branches=[(RunnableLambda(lambda x: not x.get('chat_history', False)), RunnableLambda(lambda x: x['input'])\n",
       "| VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x000001F3E8974E10>, search_kwargs={'k': 3}))], default=ChatPromptTemplate(input_variables=['chat_history', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Given a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.')), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "| ChatGoogleGenerativeAI(model='models/gemini-1.5-flash', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000001F39C2AD750>, async_client=<google.ai.generativelanguage_v1beta.services.generative_service.async_client.GenerativeServiceAsyncClient object at 0x000001F3A134D4D0>, default_metadata=())\n",
       "| StrOutputParser()\n",
       "| VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x000001F3E8974E10>, search_kwargs={'k': 3})), config={'run_name': 'chat_retriever_chain'})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_aware_retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20392ce-5e9f-41d9-a0fe-59b67f930dd9",
   "metadata": {},
   "source": [
    "1. Xác định ngữ cảnh cho câu hỏi (Contextualizing Questions)\n",
    "Phần này tập trung vào cách hệ thống xử lý các câu hỏi tiếp theo trong một cuộc trò chuyện.\n",
    "\n",
    "- contextualize_q_system_prompt: Đây là một lời nhắc hệ thống (system prompt) hướng dẫn Mô hình Ngôn ngữ Lớn (LLM) về vai trò của nó. Mục đích của nó là lấy câu hỏi hiện tại của người dùng và lịch sử trò chuyện đang diễn ra, sau đó tạo ra một câu hỏi độc lập có thể hiểu được mà không cần đến lịch sử trò chuyện. Điều này rất quan trọng vì nếu người dùng hỏi một câu hỏi tiếp theo như \"Cái đó thì sao?\", hệ thống cần hiểu \"cái đó\" đề cập đến điều gì trong ngữ cảnh của cuộc trò chuyện trước đó. Lời nhắc này đặc biệt yêu cầu LLM không trả lời câu hỏi, chỉ cần diễn đạt lại nó.\n",
    "\n",
    "- contextualize_q_prompt: Mã này tạo ra một ChatPromptTemplate sử dụng lời nhắc hệ thống. Nó cũng bao gồm MessagesPlaceholder(\"chat_history\") và (\"human\", \"{input}\").\n",
    "MessagesPlaceholder(\"chat_history\"): Điều này cho biết mẫu lời nhắc sẽ chèn lịch sử trò chuyện thực tế (một danh sách các tin nhắn) vào vị trí này khi lời nhắc được tạo.\n",
    "(\"human\", \"{input}\"): Điều này đại diện cho câu hỏi mới nhất từ người dùng. Mẫu này sẽ được sử dụng để định dạng đầu vào cho LLM khi nó cố gắng diễn đạt lại một câu hỏi.\n",
    "- history_aware_retriever: Đây là một thành phần quan trọng.\n",
    "\n",
    "- create_history_aware_retriever kết hợp:\n",
    "llm: Mô hình ngôn ngữ lớn của bạn (ví dụ: GPT của OpenAI, Gemini của Google, v.v.).\n",
    "retriever: Một đối tượng chịu trách nhiệm tìm nạp các tài liệu liên quan từ cơ sở tri thức của bạn dựa trên một truy vấn. (Đối tượng retriever không được định nghĩa trong đoạn mã bạn cung cấp nhưng được giả định là đã tồn tại).\n",
    "contextualize_q_prompt: Mẫu lời nhắc được định nghĩa ở trên. Thiết lập này có nghĩa là trước khi tìm nạp tài liệu, LLM sẽ sử dụng contextualize_q_prompt và chat_history để diễn đạt lại câu hỏi input của người dùng thành một truy vấn độc lập, rõ ràng. Truy vấn được diễn đạt lại này sau đó được chuyển đến retriever để tìm thông tin liên quan.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bc819e-9cb3-46ad-8a85-175b1e17350e",
   "metadata": {},
   "source": [
    "- qa_system_prompt: Đây là một lời nhắc hệ thống khác, nhưng lời nhắc này hướng dẫn LLM cách trả lời câu hỏi sau khi ngữ cảnh liên quan đã được tìm nạp. Nó nêu rõ:\n",
    "Vai trò của LLM là \"trợ lý trả lời câu hỏi\".\n",
    "Nó phải sử dụng context được cung cấp (sẽ được chèn vào {context}).\n",
    "Nếu câu trả lời không có trong ngữ cảnh, nó nên nói \"Tôi không biết\".\n",
    "Câu trả lời phải ngắn gọn, \"tối đa ba câu\".\n",
    "- qa_prompt: Tương tự như contextualize_q_prompt, điều này tạo ra một ChatPromptTemplate cho giai đoạn trả lời câu hỏi. Nó bao gồm qa_system_prompt, chat_history và input gốc của người dùng. Giá trị giữ chỗ {context} trong qa_system_prompt sẽ được điền bằng các tài liệu được tìm nạp bởi history_aware_retriever.\n",
    "- Youtube_chain: Điều này sử dụng create_stuff_documents_chain. Thuật ngữ \"stuff\" (nhồi nhét) đề cập đến một chiến lược phổ biến trong RAG, nơi tất cả các tài liệu đã tìm nạp được \"nhồi nhét\" (tức là nối và chèn) trực tiếp vào lời nhắc làm ngữ cảnh cho LLM. Chuỗi này lấy llm và qa_prompt để tạo câu trả lời dựa trên các tài liệu được cung cấp."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d07d6b-a67b-4c1a-bdf9-2fb4bf90f478",
   "metadata": {},
   "source": [
    "rag_chain: Đây là chuỗi Retrieval-Augmented Generation chính. create_retrieval_chain kết nối history_aware_retriever với Youtube_chain. Đây là cách nó hoạt động từ đầu đến cuối:\n",
    "- Một câu hỏi input của người dùng được đưa vào.\n",
    "- history_aware_retriever (sử dụng contextualize_q_prompt và llm) đầu tiên diễn đạt lại input thành một câu hỏi độc lập, có tính đến chat_history.\n",
    "- Câu hỏi được diễn đạt lại này sau đó được sử dụng bởi retriever (một phần của history_aware_retriever) để tìm nạp các tài liệu liên quan từ cơ sở tri thức của bạn.\n",
    "- Các tài liệu được tìm nạp này được chuyển làm context cho Youtube_chain.\n",
    "- Youtube_chain (sử dụng qa_prompt và llm) sau đó tạo ra một câu trả lời ngắn gọn cho câu hỏi input gốc, chỉ dựa vào context được cung cấp và tuân thủ các ràng buộc đã chỉ định (ví dụ: tối đa ba câu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ab542bb-e0fc-4654-947d-52a93e691863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start chatting with the AI! Type 'exit' to end the conversation.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  who is romeo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: Romeo is a fictional character and the titular protagonist of William Shakespeare's famous tragedy *Romeo and Juliet*.  He is a Montague, a member of a family locked in a bitter feud with the Capulets.  His love for Juliet, a Capulet, ultimately leads to their tragic deaths.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  do you know what i asked\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: Yes, you asked who Romeo is.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  q\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: That's not a question.  Is there something else I can help you with?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    }
   ],
   "source": [
    "continual_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3abc5f42-6717-4cfa-8310-4d0fa41ddbb6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m chat_history = [HumanMessage(\u001b[33m'\u001b[39m\u001b[33mwho is romeo\u001b[39m\u001b[33m'\u001b[39m), AIMessage(\u001b[33m\"\u001b[39m\u001b[33mRomeo is a fictional character and the titular protagonist of William Shakespeare\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms famous tragedy *Romeo and Juliet*.  He is a Montague, a member of a family locked in a bitter feud with the Capulets.  His love for Juliet, a Capulet, ultimately leads to their tragic deaths.\u001b[39m\u001b[33m\"\u001b[39m)]\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m history_aware_retriever.invoke(\u001b[33m'\u001b[39m\u001b[33mDoes he love Juliet\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\learn_langchain\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5093\u001b[39m, in \u001b[36mRunnableBindingBase.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5087\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m   5088\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   5089\u001b[39m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[32m   5090\u001b[39m     config: Optional[RunnableConfig] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   5091\u001b[39m     **kwargs: Optional[Any],\n\u001b[32m   5092\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5093\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bound.invoke(\n\u001b[32m   5094\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   5095\u001b[39m         \u001b[38;5;28mself\u001b[39m._merge_configs(config),\n\u001b[32m   5096\u001b[39m         **{**\u001b[38;5;28mself\u001b[39m.kwargs, **kwargs},\n\u001b[32m   5097\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\learn_langchain\\Lib\\site-packages\\langchain_core\\runnables\\branch.py:219\u001b[39m, in \u001b[36mRunnableBranch.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, branch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.branches):\n\u001b[32m    217\u001b[39m     condition, runnable = branch\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m     expression_value = condition.invoke(\n\u001b[32m    220\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    221\u001b[39m         config=patch_config(\n\u001b[32m    222\u001b[39m             config,\n\u001b[32m    223\u001b[39m             callbacks=run_manager.get_child(tag=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcondition:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m),\n\u001b[32m    224\u001b[39m         ),\n\u001b[32m    225\u001b[39m     )\n\u001b[32m    227\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m expression_value:\n\u001b[32m    228\u001b[39m         output = runnable.invoke(\n\u001b[32m    229\u001b[39m             \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    230\u001b[39m             config=patch_config(\n\u001b[32m   (...)\u001b[39m\u001b[32m    234\u001b[39m             **kwargs,\n\u001b[32m    235\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\learn_langchain\\Lib\\site-packages\\langchain_core\\runnables\\base.py:4475\u001b[39m, in \u001b[36mRunnableLambda.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   4461\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Invoke this Runnable synchronously.\u001b[39;00m\n\u001b[32m   4462\u001b[39m \n\u001b[32m   4463\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   4472\u001b[39m \u001b[33;03m    TypeError: If the Runnable is a coroutine function.\u001b[39;00m\n\u001b[32m   4473\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfunc\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m4475\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_with_config(\n\u001b[32m   4476\u001b[39m         \u001b[38;5;28mself\u001b[39m._invoke,\n\u001b[32m   4477\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   4478\u001b[39m         \u001b[38;5;28mself\u001b[39m._config(config, \u001b[38;5;28mself\u001b[39m.func),\n\u001b[32m   4479\u001b[39m         **kwargs,\n\u001b[32m   4480\u001b[39m     )\n\u001b[32m   4481\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   4482\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m   4483\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot invoke a coroutine function synchronously.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4484\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUse `ainvoke` instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4485\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\learn_langchain\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1786\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m     context = copy_context()\n\u001b[32m   1783\u001b[39m     context.run(_set_config_context, child_config)\n\u001b[32m   1784\u001b[39m     output = cast(\n\u001b[32m   1785\u001b[39m         Output,\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m         context.run(\n\u001b[32m   1787\u001b[39m             call_func_with_variable_args,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m   1788\u001b[39m             func,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m   1789\u001b[39m             \u001b[38;5;28minput\u001b[39m,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m   1790\u001b[39m             config,\n\u001b[32m   1791\u001b[39m             run_manager,\n\u001b[32m   1792\u001b[39m             **kwargs,\n\u001b[32m   1793\u001b[39m         ),\n\u001b[32m   1794\u001b[39m     )\n\u001b[32m   1795\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1796\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\learn_langchain\\Lib\\site-packages\\langchain_core\\runnables\\config.py:398\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    397\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28minput\u001b[39m, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\learn_langchain\\Lib\\site-packages\\langchain_core\\runnables\\base.py:4331\u001b[39m, in \u001b[36mRunnableLambda._invoke\u001b[39m\u001b[34m(self, input, run_manager, config, **kwargs)\u001b[39m\n\u001b[32m   4329\u001b[39m                 output = chunk\n\u001b[32m   4330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4331\u001b[39m     output = call_func_with_variable_args(\n\u001b[32m   4332\u001b[39m         \u001b[38;5;28mself\u001b[39m.func, \u001b[38;5;28minput\u001b[39m, config, run_manager, **kwargs\n\u001b[32m   4333\u001b[39m     )\n\u001b[32m   4334\u001b[39m \u001b[38;5;66;03m# If the output is a Runnable, invoke it\u001b[39;00m\n\u001b[32m   4335\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\learn_langchain\\Lib\\site-packages\\langchain_core\\runnables\\config.py:398\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    397\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28minput\u001b[39m, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\learn_langchain\\Lib\\site-packages\\langchain\\chains\\history_aware_retriever.py:60\u001b[39m, in \u001b[36mcreate_history_aware_retriever.<locals>.<lambda>\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m prompt.input_variables:\n\u001b[32m     52\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     53\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mExpected `input` to be a prompt variable, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     54\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt.input_variables\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     55\u001b[39m     )\n\u001b[32m     57\u001b[39m retrieve_documents: RetrieverOutputLike = RunnableBranch(\n\u001b[32m     58\u001b[39m     (\n\u001b[32m     59\u001b[39m         \u001b[38;5;66;03m# Both empty string and empty list evaluate to False\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;129;01mnot\u001b[39;00m x.get(\u001b[33m\"\u001b[39m\u001b[33mchat_history\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m     61\u001b[39m         \u001b[38;5;66;03m# If no chat history, then we just pass input to retriever\u001b[39;00m\n\u001b[32m     62\u001b[39m         (\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m]) | retriever,\n\u001b[32m     63\u001b[39m     ),\n\u001b[32m     64\u001b[39m     \u001b[38;5;66;03m# If chat history, then we pass inputs to LLM chain, then to retriever\u001b[39;00m\n\u001b[32m     65\u001b[39m     prompt | llm | StrOutputParser() | retriever,\n\u001b[32m     66\u001b[39m ).with_config(run_name=\u001b[33m\"\u001b[39m\u001b[33mchat_retriever_chain\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m retrieve_documents\n",
      "\u001b[31mAttributeError\u001b[39m: 'str' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "chat_history = [HumanMessage('who is romeo'), AIMessage(\"Romeo is a fictional character and the titular protagonist of William Shakespeare's famous tragedy *Romeo and Juliet*.  He is a Montague, a member of a family locked in a bitter feud with the Capulets.  His love for Juliet, a Capulet, ultimately leads to their tragic deaths.\")]\n",
    "history_aware_retriever.invoke({'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2a8f41-d02d-4b8f-9864-24badf6b7152",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn_langchain",
   "language": "python",
   "name": "learn_langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
